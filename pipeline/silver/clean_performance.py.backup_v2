"""
Silver Layer - Performance Transformation
NOW READS FROM MINIO BRONZE
"""
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import (
    col, row_number, current_timestamp,
    when, lit, round as spark_round
)
from config.minio_config import minio_config
from pipeline.utils.spark_postgres import get_postgres_properties


def create_spark_session():
    """Create Spark session with MinIO support."""
    builder = SparkSession.builder \
        .appName("SilverLayer-Performance") \
        .config("spark.jars.packages",
                "org.apache.hadoop:hadoop-aws:3.3.4,"
                "com.amazonaws:aws-java-sdk-bundle:1.12.262,"
                "org.postgresql:postgresql:42.6.0")
    
    for key, value in minio_config.get_spark_config().items():
        builder = builder.config(key, value)
    
    return builder.getOrCreate()


def safe_divide(numerator, denominator, decimal_places=2):
    """Safe division that handles division by zero."""
    return when(col(denominator) == 0, lit(0.0)) \
        .otherwise(spark_round(col(numerator) / col(denominator), decimal_places))


def clean_performance():
    """
    Transform Bronze performance to Silver.
    
    Pipeline:
        MinIO Bronze ‚Üí Transformations ‚Üí MinIO Silver + Postgres Silver
    
    Transformations:
        - Type casting
        - Deduplication
        - Calculated metrics (CTR, CPC, CVR, CPA)
        - Data quality validation
    """
    spark = create_spark_session()
    
    try:
        print("=" * 80)
        print("SILVER LAYER TRANSFORMATION - PERFORMANCE")
        print("=" * 80)
        
        # ===== READ FROM MINIO BRONZE =====
        bronze_path = minio_config.get_s3_path("bronze", "performance/")
        print(f"\nüìÇ Reading from MinIO Bronze: {bronze_path}")
        
        raw_df = spark.read.parquet(bronze_path)
        raw_count = raw_df.count()
        print(f"‚úÖ Loaded {raw_count} records from Bronze")
        
        # ===== TYPE CASTING =====
        print("\nüîß Applying transformations...")
        print("   Step 1: Type casting")
        
        typed_df = raw_df \
            .withColumn("performance_id", col("performance_id").cast("int")) \
            .withColumn("campaign_id", col("campaign_id").cast("int")) \
            .withColumn("performance_date", col("performance_date").cast("date")) \
            .withColumn("impressions", col("impressions").cast("bigint")) \
            .withColumn("clicks", col("clicks").cast("bigint")) \
            .withColumn("conversions", col("conversions").cast("int")) \
            .withColumn("spend", col("spend").cast("decimal(15,2)")) \
            .withColumn("revenue", col("revenue").cast("decimal(15,2)"))
        
        # ===== DEDUPLICATION =====
        print("   Step 2: Deduplication")
        
        window_spec = Window.partitionBy("performance_id") \
            .orderBy(col("_ingestion_timestamp").desc())
        
        deduped_df = typed_df \
            .withColumn("rn", row_number().over(window_spec)) \
            .filter(col("rn") == 1) \
            .drop("rn")
        
        deduped_count = deduped_df.count()
        duplicates_removed = raw_count - deduped_count
        print(f"      Removed {duplicates_removed} duplicate records")
        
        # ===== CALCULATED METRICS =====
        print("   Step 3: Calculated metrics (CTR, CPC, CVR, CPA)")
        
        silver_df = deduped_df \
            .withColumn("ctr", safe_divide("clicks", "impressions", 4) * 100) \
            .withColumn("cvr", safe_divide("conversions", "clicks", 4) * 100) \
            .withColumn("cpc", safe_divide("spend", "clicks", 2)) \
            .withColumn("cpa", safe_divide("spend", "conversions", 2)) \
            .withColumn("_silver_processed_at", current_timestamp())
        
        print(f"      Added: ctr, cvr, cpc, cpa")
        
        # ===== WRITE TO MINIO SILVER =====
        silver_path = minio_config.get_s3_path("silver", "performance/")
        print(f"\nüíæ Writing to MinIO Silver: {silver_path}")
        print(f"   Partitioning by: performance_date")
        
        silver_df.write \
            .mode("overwrite") \
            .partitionBy("performance_date") \
            .parquet(silver_path)
        
        print(f"‚úÖ MinIO Silver write successful")
        
        # ===== WRITE TO POSTGRES SILVER =====
        postgres_props = get_postgres_properties()
        print(f"\nüíæ Writing to Postgres: silver.performance")
        
        silver_df.write.jdbc(
            url=postgres_props["url"],
            table="silver.performance",
            mode="overwrite",
            properties=postgres_props
        )
        
        print(f"‚úÖ Postgres Silver write successful")
        
        # ===== SUMMARY =====
        print("\n" + "=" * 80)
        print("üìä TRANSFORMATION SUMMARY")
        print("=" * 80)
        print(f"Input Records:      {raw_count}")
        print(f"Output Records:     {deduped_count}")
        print(f"Duplicates Removed: {duplicates_removed}")
        print(f"MinIO Path:         {silver_path}")
        print(f"Partitioning:       performance_date")
        print(f"Status:             ‚úÖ SUCCESS")
        print("=" * 80 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        spark.stop()


if __name__ == "__main__":
    clean_performance()
# Campaign Analytics Lakehouse

Production-grade data lakehouse implementing Delta Lake, medallion architecture, and dbt transformations.

## ğŸ—ï¸ Architecture
```
CSV Files â†’ Bronze (Delta) â†’ Silver (Delta) â†’ Gold (dbt) â†’ Postgres
              â†“                  â†“               â†“
           MinIO              MinIO         Postgres
```

### Key Features
- âœ… **Delta Lake**: ACID transactions, time travel, MERGE operations
- âœ… **Medallion Architecture**: Bronze â†’ Silver â†’ Gold layers
- âœ… **Data Quality**: 110+ automated dbt tests
- âœ… **Incremental Processing**: Partition-based incremental loads
- âœ… **Containerized**: Full Docker Compose setup

## ğŸ“Š Layer Details

### Bronze Layer (Raw Archive)
- **Purpose**: Immutable raw data archive
- **Format**: Delta Lake (Parquet + transaction log)
- **Partitioning**: `ingest_date` for incremental processing
- **Location**: MinIO `s3a://bronze/{table}/`
- **Schema**: All columns as STRING (raw format)

**Example:**
```
bronze/campaigns/
â”œâ”€â”€ ingest_date=2026-02-19/
â”‚   â””â”€â”€ part-00000.snappy.parquet
â””â”€â”€ _delta_log/
    â””â”€â”€ 00000000000.json
```

### Silver Layer (Curated Data)
- **Purpose**: Cleaned, validated, business-ready data
- **Format**: Delta Lake
- **Partitioning**: Business attributes (status, date, industry)
- **Location**: MinIO `s3a://silver/{table}/`
- **Transformations**: Type casting, deduplication, validation

**Example:**
```
silver/campaigns/
â”œâ”€â”€ status=Active/
â”‚   â””â”€â”€ part-00000.snappy.parquet
â”œâ”€â”€ status=Paused/
â””â”€â”€ _delta_log/
```

### Gold Layer (Analytics)
- **Purpose**: Star schema for BI/analytics
- **Tool**: dbt
- **Location**: Postgres `core.*`, `analytics.*`
- **Models**: 9 models (staging â†’ intermediate â†’ marts)
- **Tests**: 110+ data quality tests

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.12+
- 8GB RAM minimum

### Setup
```bash
# 1. Clone repository
git clone https://github.com/Franklin0603/campaign_analytics_pipeline.git
cd campaign_analytics_pipeline

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start infrastructure
docker-compose up -d

# 5. Run full pipeline
python run_full_pipeline.py
```

### Verify
```bash
# Check MinIO Console
open http://localhost:9001  # Login: minioadmin/minioadmin123

# Check Postgres data
docker exec -it campaign_analytics_db psql -U dbt_user -d campaign_analytics -c "SELECT COUNT(*) FROM core.dim_campaigns;"

# View dbt docs
cd dbt_project
dbt docs generate
dbt docs serve
```

## ğŸ“ Project Structure
```
campaign_analytics_pipeline/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database.yaml          # Database connection configs
â”‚   â””â”€â”€ minio_config.py        # MinIO S3 configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                   # Source CSV files
â”‚       â”œâ”€â”€ campaigns.csv
â”‚       â”œâ”€â”€ performance.csv
â”‚       â””â”€â”€ advertisers.csv
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ bronze/                # Bronze layer ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_campaigns.py
â”‚   â”‚   â”œâ”€â”€ ingest_performance.py
â”‚   â”‚   â””â”€â”€ ingest_advertisers.py
â”‚   â”œâ”€â”€ silver/                # Silver layer transformations
â”‚   â”‚   â”œâ”€â”€ clean_campaigns.py
â”‚   â”‚   â”œâ”€â”€ clean_performance.py
â”‚   â”‚   â””â”€â”€ clean_advertisers.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ spark_postgres.py  # Shared utilities
â”œâ”€â”€ dbt_project/               # Gold layer (dbt)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/           # Source data staging
â”‚   â”‚   â”œâ”€â”€ intermediate/      # Business logic
â”‚   â”‚   â””â”€â”€ marts/             # Final analytics tables
â”‚   â””â”€â”€ tests/                 # Data quality tests
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_db.sql            # Database initialization
â”œâ”€â”€ docker-compose.yml         # Infrastructure setup
â”œâ”€â”€ run_full_pipeline.py       # Master pipeline runner
â”œâ”€â”€ demo_delta_features.py     # Delta Lake capabilities demo
â””â”€â”€ requirements.txt
```

## ğŸ’¾ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Object Storage | MinIO | S3-compatible data lake |
| Table Format | Delta Lake 2.4 | ACID transactions, versioning |
| Processing | Apache Spark 3.4 | Distributed data processing |
| Warehouse | PostgreSQL 15 | Analytics database |
| Transformation | dbt 1.9 | SQL-based modeling |
| Data Quality | Great Expectations | Validation framework |
| Orchestration | Python scripts | Pipeline automation |
| Infrastructure | Docker Compose | Containerization |

## ğŸ“ˆ Data Flow

### Full Pipeline
```bash
python run_full_pipeline.py
```

**Execution:**
1. **Bronze**: Ingest CSV â†’ Delta Lake (MinIO + Postgres)
2. **Silver**: Transform â†’ Delta Lake (MinIO + Postgres)
3. **Gold**: dbt run â†’ Star schema (Postgres)

**Duration:** ~60-90 seconds

### Individual Layers
```bash
# Bronze only
python pipeline/bronze/ingest_campaigns.py

# Silver only
python pipeline/silver/clean_campaigns.py

# Gold only (dbt)
cd dbt_project
dbt run
dbt test
```

## ğŸ¯ Delta Lake Features

### Time Travel
```python
# Query historical versions
df = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .load("s3a://silver/campaigns/")

# Query by timestamp
df = spark.read.format("delta") \
    .option("timestampAsOf", "2026-02-19") \
    .load("s3a://silver/campaigns/")
```

### MERGE Operations
```python
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "s3a://silver/campaigns/")

# Update existing, insert new
deltaTable.alias("target").merge(
    updates.alias("source"),
    "target.campaign_id = source.campaign_id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

### Version History
```python
# Run demo script
python demo_delta_features.py
```

## ğŸ§ª Data Quality

### dbt Tests (110+ tests)
```bash
cd dbt_project
dbt test

# Run specific test
dbt test --select dim_campaigns
```

**Test categories:**
- âœ… Uniqueness constraints
- âœ… Not null checks
- âœ… Referential integrity
- âœ… Accepted values
- âœ… Custom business rules

### Great Expectations
```bash
# Run validation (integrated in Silver layer)
python pipeline/silver/clean_campaigns.py
```

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| Storage Reduction | 70% (Parquet vs CSV) |
| Bronze Ingestion | ~15 seconds |
| Silver Transform | ~20 seconds |
| Gold dbt Run | ~25 seconds |
| **Total Pipeline** | **~60 seconds** |
| Data Quality Tests | 110+ automated |

## ğŸ”§ Configuration

### Environment Variables

Create `.env` file:
```bash
# Postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=campaign_analytics
POSTGRES_USER=dbt_user
POSTGRES_PASSWORD=dbt_password

# MinIO
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_SECURE=false

# JDBC
JDBC_DRIVER_PATH=lib/postgresql-42.6.0.jar
```

### MinIO Buckets

Created automatically on startup:
- `bronze` - Raw data archive
- `silver` - Curated data
- `logs` - Pipeline logs
- `data-quality` - Quality reports

## ğŸ“– Documentation

- [Architecture Guide](docs/ARCHITECTURE.md) - Detailed system design
- [Lakehouse Guide](docs/LAKEHOUSE_GUIDE.md) - Delta Lake operations
- [dbt Documentation](http://localhost:8080) - Run `dbt docs serve`

## ğŸ› Troubleshooting

### Docker containers not starting
```bash
# Check what's using port 5432
lsof -i :5432

# Stop local PostgreSQL
brew services stop postgresql

# Restart containers
docker-compose down
docker-compose up -d
```

### Pipeline fails
```bash
# Check container logs
docker logs campaign_analytics_db
docker logs campaign_minio

# Verify MinIO is accessible
docker exec campaign_minio mc ls local/

# Check Postgres connection
docker exec -it campaign_analytics_db psql -U dbt_user -d campaign_analytics
```

### Delta Lake "table doesn't exist"
```bash
# Ensure Bronze ran first
python pipeline/bronze/ingest_campaigns.py

# Then run Silver
python pipeline/silver/clean_campaigns.py
```

## ğŸ¤ Contributing

This is a portfolio project. Feedback and suggestions welcome via issues.

## ğŸ“ License

MIT License - see LICENSE file

## ğŸ‘¤ Author

**Franklin Ajisogun**
- LinkedIn: [franklin-ajisogun](https://linkedin.com/in/franklin-ajisogun)
- GitHub: [@Franklin0603](https://github.com/Franklin0603)
- Portfolio: [Your Portfolio URL]

## ğŸ™ Acknowledgments

- Built with guidance from industry best practices
- Inspired by production lakehouse implementations at Uber, Netflix, Airbnb
- Delta Lake technology from Linux Foundations
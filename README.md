# Campaign Analytics Pipeline

> **End-to-end data pipeline** implementing Bronze/Silver/Gold (Medallion) architecture with PySpark, dbt, and Great Expectations

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PySpark](https://img.shields.io/badge/PySpark-3.4.0-orange.svg)](https://spark.apache.org/)
[![dbt](https://img.shields.io/badge/dbt-1.8+-red.svg)](https://www.getdbt.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)](https://www.postgresql.org/)

## ğŸ“‹ Overview

This project demonstrates a **production-ready data pipeline** for campaign analytics, showcasing:

- âœ… **Medallion Architecture** (Bronze â†’ Silver â†’ Gold)
- âœ… **PySpark** for distributed data processing
- âœ… **dbt** for analytics engineering
- âœ… **Great Expectations** for data quality validation
- âœ… **PostgreSQL** as the data warehouse
- âœ… **Modular, testable, and maintainable** code structure

### Architecture

```
CSV Files (Raw Data)
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER   â”‚  â† Raw data preservation
â”‚  (PySpark)      â”‚     â€¢ Campaigns
â”‚                 â”‚     â€¢ Performance
â”‚  PostgreSQL     â”‚     â€¢ Advertisers
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER   â”‚  â† Data transformation & quality
â”‚  (PySpark +     â”‚     â€¢ Type casting
â”‚   Great Exp.)   â”‚     â€¢ Deduplication
â”‚                 â”‚     â€¢ Business rules
â”‚  PostgreSQL     â”‚     â€¢ Quality checks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GOLD LAYER    â”‚  â† Analytics-ready models
â”‚   (dbt)         â”‚     â€¢ Dimension tables
â”‚                 â”‚     â€¢ Fact tables
â”‚  PostgreSQL     â”‚     â€¢ Aggregations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PostgreSQL 15+
- Java 8+ (for PySpark)
- 2GB RAM minimum

### Installation

```bash
# 1. Clone the repository
git clone <your-repo-url>
cd campaign_analytics_pipeline

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download PostgreSQL JDBC driver
mkdir lib
cd lib
curl -O https://jdbc.postgresql.org/download/postgresql-42.6.0.jar
cd ..

# 5. Setup PostgreSQL database
createdb campaign_analytics
psql campaign_analytics < sql/create_schemas.sql
```

### Configuration

Edit `config/database.yaml` with your PostgreSQL credentials:

```yaml
postgres:
  host: localhost
  port: 5432
  database: campaign_analytics
  user: postgres
  password: "your_password"  # Add your password
```

### Run the Pipeline

```bash
# Run Bronze â†’ Silver layers
python run_pipeline.py

# Initialize dbt (first time only)
dbt init dbt_project
# Follow prompts with your PostgreSQL credentials

# Run dbt models (Gold layer)
cd dbt_project
dbt run
dbt test
dbt docs generate
dbt docs serve
```

## ğŸ“ Project Structure

```
campaign_analytics_pipeline/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                    # Sample CSV data
â”‚       â”œâ”€â”€ campaigns.csv
â”‚       â”œâ”€â”€ performance.csv
â”‚       â””â”€â”€ advertisers.csv
â”œâ”€â”€ pyspark/
â”‚   â”œâ”€â”€ bronze/                 # Bronze layer ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_campaigns.py
â”‚   â”‚   â”œâ”€â”€ ingest_performance.py
â”‚   â”‚   â””â”€â”€ ingest_advertisers.py
â”‚   â”œâ”€â”€ silver/                 # Silver layer transformations
â”‚   â”‚   â”œâ”€â”€ clean_campaigns.py
â”‚   â”‚   â”œâ”€â”€ clean_performance.py
â”‚   â”‚   â””â”€â”€ clean_advertisers.py
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”‚       â””â”€â”€ spark_postgres.py
â”œâ”€â”€ dbt_project/                # dbt analytics models
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/           # Staging views
â”‚   â”‚   â”œâ”€â”€ intermediate/      # Business logic
â”‚   â”‚   â””â”€â”€ marts/             # Final dimensions & facts
â”‚   â””â”€â”€ tests/                 # dbt tests
â”œâ”€â”€ config/
â”‚   â””â”€â”€ database.yaml          # Database configuration
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_schemas.sql     # Schema setup
â”œâ”€â”€ quality_reports/           # Data quality reports
â”œâ”€â”€ run_pipeline.py            # Master pipeline runner
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md
```

## ğŸ”„ Pipeline Stages

### Bronze Layer (Raw Data)
- **Purpose**: Preserve raw data exactly as received
- **Technology**: PySpark
- **Schema**: `bronze.raw_*`
- **Features**:
  - No transformations
  - All columns as STRING type
  - Metadata columns for lineage
  - Append-only writes

### Silver Layer (Cleansed Data)
- **Purpose**: Transform and validate data
- **Technology**: PySpark + Great Expectations
- **Schema**: `silver.*`
- **Features**:
  - Type casting and validation
  - Deduplication
  - Business rule application
  - Data quality checks
  - Calculated fields (CTR, CPC, etc.)

### Gold Layer (Analytics)
- **Purpose**: Business-ready analytics models
- **Technology**: dbt
- **Schema**: `analytics.*`
- **Features**:
  - Dimension tables
  - Fact tables with joins
  - Aggregations
  - Tests and documentation

## ğŸ“Š Data Model

### Source Tables (Bronze)
- `bronze.raw_campaigns` - Campaign master data
- `bronze.raw_performance` - Daily performance metrics
- `bronze.raw_advertisers` - Advertiser information

### Cleansed Tables (Silver)
- `silver.campaigns` - Validated campaigns with derived fields
- `silver.performance` - Metrics with CTR, CPC, conversion rates
- `silver.advertisers` - Standardized advertiser data

### Analytics Tables (Gold)
- `analytics.dim_campaigns` - Campaign dimension
- `analytics.dim_advertisers` - Advertiser dimension
- `analytics.fact_performance` - Performance fact table

## ğŸ” Data Quality

The pipeline includes comprehensive data quality checks using **Great Expectations**:

- âœ… Schema validation
- âœ… Null checks
- âœ… Uniqueness constraints
- âœ… Range validation
- âœ… Business rule enforcement
- âœ… Referential integrity

Quality check results are logged and can fail the pipeline if critical issues are detected.

## ğŸ§ª Testing

```bash
# Test Bronze ingestion
python pyspark/bronze/ingest_campaigns.py

# Test Silver transformation
python pyspark/silver/clean_campaigns.py

# Test dbt models
cd dbt_project
dbt test

# Verify data in PostgreSQL
psql campaign_analytics -c "SELECT COUNT(*) FROM silver.campaigns;"
psql campaign_analytics -c "SELECT * FROM analytics.dim_campaigns LIMIT 5;"
```

## ğŸ“ˆ Key Metrics Calculated

The pipeline automatically calculates:

- **Click-Through Rate (CTR)**: `(clicks / impressions) Ã— 100`
- **Conversion Rate**: `(conversions / clicks) Ã— 100`
- **Cost Per Click (CPC)**: `spend / clicks`
- **Cost Per Conversion**: `spend / conversions`
- **Campaign Duration**: `end_date - start_date`
- **Budget Tier**: Small (<$10K), Medium (<$50K), Large (>$50K)

## ğŸ› ï¸ Technologies Used

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Data Processing** | PySpark 3.4 | Distributed data transformation |
| **Analytics** | dbt 1.8 | SQL-based modeling |
| **Data Quality** | Great Expectations | Validation framework |
| **Database** | PostgreSQL 15 | Data warehouse |
| **Language** | Python 3.8+ | Pipeline orchestration |

## ğŸ¯ Use Cases

This pipeline architecture is suitable for:

- Marketing analytics platforms
- Campaign performance tracking
- Multi-source data integration
- Data quality monitoring
- Analytics engineering workflows

## ğŸ“ Development Notes

### Adding New Data Sources

1. Create CSV in `data/raw/`
2. Add ingestion script in `pyspark/bronze/`
3. Add transformation in `pyspark/silver/`
4. Create dbt model in `dbt_project/models/`

### Extending Transformations

Silver layer transformations can be extended with:
- Additional calculated fields
- More complex business rules
- Advanced data quality checks
- ML feature engineering

## ğŸ¤ Contributing

Contributions welcome! Areas for enhancement:
- Airflow/Prefect orchestration
- Delta Lake integration
- Advanced aggregations
- Real-time streaming ingestion
- CI/CD pipeline

## ğŸ“§ Contact

**Franklin Ajisogun**
- LinkedIn: [your-linkedin]
- GitHub: [your-github]
- Email: [your-email]

## ğŸ“„ License

MIT License - feel free to use this project for learning and portfolio purposes.

---

**Built with** â¤ï¸ **by Franklin Ajisogun** | Demonstrating modern data engineering practices